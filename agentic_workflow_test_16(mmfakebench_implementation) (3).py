# -*- coding: utf-8 -*-
"""Agentic Workflow Test 16(MMFakeBench_Implementation).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HoRrlxQllKTC5VCqFfM9569XcxN2Dsci

# üì∞ MMFakeBench: Misinformation Detection Agentic Workflow Demo
This notebook demonstrates a **multi-step, agentic misinformation detection pipeline** using multimodal LLMs (OpenAI GPT-4o, Gemini), web search, and RAG-style evidence synthesis.  
It is designed for **Colab** (Google Colaboratory) and works with provided MOCHEG news image/headline datasets.

---

## üöÄ Quick Start (Colab)

1. **Upload your API keys** (recommended: Colab *Secrets* panel):  
   - **OpenAI API Key** (`OPENAI_API_KEY`): For GPT-4o, GPT-4, GPT-3.5 etc  
   - **Brave Search Key** (`SEARCH_API`): For web search  
   - **Google Gemini API Key** (`GEMINI_API_KEY`): For Gemini models  
   Learn more: [OpenAI Keys](https://platform.openai.com/), [Brave Keys](https://search.brave.com/api), [Gemini Keys](https://aistudio.google.com/app/apikey)
```
2. **Mount Google Drive:**  
   - Put your MOCHEG data under `/MyDrive/MOCHEG/extracted/mocheg/`  
   - If you need sample data, ask the instructor or download [here](https://example.com)  
  ```
3. **Install dependencies:**  
   The notebook does this automatically (`!pip install ...`), but you can re-run the install cell if needed.
  ```
4. **Check/Set API keys in notebook (if not using Colab secrets):**
   ```python
   import os
   os.environ['OPENAI_API_KEY'] = "sk-..."      # your OpenAI key
   os.environ['SEARCH_API'] = "live..."         # your Brave search API
   os.environ['GEMINI_API_KEY'] = "..."         # your Gemini API
   ```

5. **Choose your model:**  
   At the bottom of the notebook, call:
   ```
   main(model_name="gpt-4o-mini", ...)
   ```
   or
   ```
   main(model_name="gemini-2.0-flash", ...)
   ```

6. **Run the notebook:**  
   Walk through the cells in order.  
   The pipeline will...
   - Load and filter the dataset
   - Run multimodal headline-image relevance checks
   - Iteratively generate and answer fact-checking questions via the web
   - Synthesize a final "Misinformation" or "Not Misinformation" decision

---

### üõ†Ô∏è Troubleshooting

- **Missing API Key:**  
   If you see "Missing bearer or basic authentication in header", double-check your API key setup for OpenAI, Brave, and/or Gemini.
- **429 Too Many Requests:**  
   You have hit your model's rate or usage quota. Reduce concurrent jobs, slow down requests, or check your plan limits on your model provider dashboard.
- **Dataset Issues:**  
   If the dataset loads with 0 items, check your file paths and Google Drive mount.

---

### ‚öôÔ∏è Notebook Parameters and Customization

You can modify:
- **Number of topics/samples** in `dataset_config`
- **Number of Q&A branches** in `pipeline_params`
- **Which model to use** in the `main()` call

---

### üìÑ About

- **Authors:** Mir Nafis Sharear Shopnil
- **Repository:** [github.com/your/repo](https://github.com/your/repo)
- **Inspired by:** MOCHEG, LangChain, OpenAI, Google Gemini

---

*This notebook is for academic/research use. Always secure your API keys and consult the [OpenAI](https://platform.openai.com/account/limits), [Brave](https://search.brave.com/api), or [Google AI Studio](https://aistudio.google.com/app/apikey) docs for up-to-date rate limits.*
"""

# !pip install -U duckduckgo_search

# Install necessary packages (run once)
# !pip install langchain openai python-dotenv torch transformers langchain-openai langchain-community google-search-results scikit-learn pandas google-generativeai langchain_google_genai

from typing import Tuple, List, Dict, Any, Optional, Union
import os
import re
import pandas as pd
import time
import numpy as np
import json
import logging
import base64
from PIL import Image
from duckduckgo_search import DDGS
from io import BytesIO
from IPython.display import display, Image as IPImage
import imghdr
import math
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Import Langchain and related components
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.chains import LLMChain
from pydantic import BaseModel, Field
from langchain_community.utilities.brave_search import BraveSearchWrapper
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models import BaseChatModel

# Environment and Setup
from dotenv import load_dotenv
from google.colab import drive, userdata
from torch.utils.data import Dataset, DataLoader as TorchDataLoader

# --- Environment Setup ---
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

try:
    drive.mount('/content/drive', force_remount=True)
    logging.info("Google Drive mounted successfully.")
except Exception as e:
    logging.error(f"Error mounting Google Drive: {e}")

# N_TOPICS = 30
# SEED     = 42

# import pandas as pd
# import random

# # point this at your actual CSV
# img_evidence_csv = "/content/drive/MyDrive/mocheg/train/img_evidence_qrels.csv"

# all_topics = (
#     pd.read_csv(img_evidence_csv, usecols=["TOPIC"])
#       ["TOPIC"]
#       .dropna()
#       .unique()
#       .tolist()
# )
# random.seed(SEED)
# selected_topics = random.sample(all_topics, N_TOPICS)
# print("Will run on these topics:", selected_topics)

# from shutil import copy2
# import tempfile
# import os
# import pandas as pd

# # paths and your selected_topics list from above
# base_images = "/content/drive/MyDrive/mocheg/images"
# img_qrels   = "/content/drive/MyDrive/mocheg/train/img_evidence_qrels.csv"

# # pull filenames for only your topics
# df = pd.read_csv(img_qrels, usecols=["TOPIC", "DOCUMENT#"])
# df = df[df["TOPIC"].isin(selected_topics)]
# df["doc_id"] = df["DOCUMENT#"].astype(str).str.split(r"[?&/]").str[-1]
# filenames = {f for f in os.listdir(base_images)
#              if any(f.startswith(doc) for doc in df["doc_id"])}

# temp_dir = tempfile.mkdtemp(prefix="mocheg_sel_")
# for fn in filenames:
#     src = os.path.join(base_images, fn)
#     dst = os.path.join(temp_dir, fn)
#     try:
#         copy2(src, dst)
#     except Exception as e:
#         print(f"Failed to copy {fn}: {e}")
# print("Temp folder ready:", temp_dir)

# print(os.listdir(temp_dir))

try:
    openai_key = userdata.get('OPENAI_API_KEY')
    brave_key = userdata.get('SEARCH_API')
    google_key = userdata.get('GEMINI_API_KEY')  # New key for Gemini

    if not openai_key:
        openai_key = os.getenv('OPENAI_API_KEY')
    if not brave_key:
        brave_key = os.getenv('SEARCH_API')
    if not google_key:
        google_key = os.getenv('GOOGLE_API_KEY')

    if not openai_key and not google_key:
        raise ValueError("Missing API Keys. Set OpenAI or Google API Key in Colab secrets or environment variables.")
    if not brave_key:
        raise ValueError("Missing Brave Search API Key. Set it in Colab secrets or environment variables.")
    logging.info("API keys loaded.")

except Exception as e:
    logging.error(f"Error accessing Colab secrets or environment variables: {e}")
    raise

class ModelRouter:
    def __init__(self, model_name, openai_api_key=None, google_api_key=None, temperature=0.2, max_retries=5):
        self.model_name = model_name.lower()
        self.openai_api_key = openai_api_key
        self.google_api_key = google_api_key
        self.temperature = temperature
        self.max_retries = max_retries
        self._llm = self._init_llm()

    def _init_llm(self):
        if 'gpt' in self.model_name:
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(api_key=self.openai_api_key, model=self.model_name, temperature=self.temperature)
        elif 'gemini' in self.model_name:
            from langchain_google_genai import ChatGoogleGenerativeAI
            return ChatGoogleGenerativeAI(model=self.model_name, google_api_key=self.google_api_key, temperature=self.temperature)
        else:
            raise ValueError(f"Unsupported model: {self.model_name}")

    def get_model(self):
        return self._llm

    def switch_model(self, new_model_name):
        self.model_name = new_model_name.lower()
        self._llm = self._init_llm()
        logging.info(f"Switched to model: {self.model_name}")

    def encode_image_for_model(self, image_path):
        return encode_image(image_path)

    def create_multimodal_message(self, system_prompt, text_prompt, image_path):
        base64_image, mime_type = self.encode_image_for_model(image_path)
        if not base64_image:
            raise ValueError(f"Failed to encode image: {image_path}")

        from langchain_core.messages import HumanMessage, SystemMessage

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=[
                {"type": "text", "text": text_prompt},
                {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}}
            ])
        ]
        return messages


    def call_model(self, messages):
        tries = 0
        while tries < self.max_retries:
            try:
                response = self._llm.invoke(messages)
                return response
            except Exception as e:
                msg = str(e).lower()
                if ("rate limit" in msg or "429" in msg or "quota" in msg or "please try again" in msg):
                    sleep_time = 2 ** tries
                    logging.warning(f"[Router] Rate limit or quota error on try {tries+1}/{self.max_retries}. Sleeping {sleep_time}s ‚Ä¶")
                    time.sleep(sleep_time)
                    tries += 1
                    continue
                elif "image format" in msg or "unsupported image" in msg:
                    logging.error("[Router] Unsupported/corrupt image payload sent to model.")
                    return None
                else:
                    logging.error(f"[Router] Model call failed ({type(e)}): {e}")
                    return None
        logging.error("[Router] Max retries exhausted.")
        return None

    # --- The key "one-way" multimodal call function for all pipeline modules ---
    def llm_multimodal(self, system_prompt, text, image_path):
        messages = self.create_multimodal_message(system_prompt, text, image_path)
        return self.call_model(messages)

# # ---------------------------------------------------------------------------
# # Headline Extractor Module (No changes needed)
# # ---------------------------------------------------------------------------
# def extract_headline_and_source(text: str):
#     if pd.isna(text) or not isinstance(text, str):
#         return "", ""
#     patterns = [
#         re.compile(
#             r'^(?P<headline>.*?)\s*'
#             r'(?P<source>'
#             r'(?P<author>[A-Z][a-zA-Z]+(?: [A-Z][a-zA-Z]+)*)'
#             r'\s+Published\s+'
#             r'(?P<date>\d{1,2}\s+[A-Za-z]+\s+\d{4})'
#             r')$',
#             re.DOTALL
#         ),
#         re.compile(
#             r'^(?P<headline>.*?)\s*'
#             r'\[?(?P<source>[A-Z][a-zA-Z ]+)\]?\s*[\-‚Äì]\s*'
#             r'(?P<date>\d{1,2}\s+[A-Za-z]+\s+\d{4})?$',
#             re.DOTALL
#         )
#     ]
#     text = text.strip()
#     for pattern in patterns:
#         match = pattern.search(text)
#         if match:
#             groups = match.groupdict()
#             # Ensure headline is not just whitespace if source is found
#             headline = groups.get('headline', '').strip()
#             source = groups.get('source', '').strip()
#             if headline: # Return only if headline is non-empty
#                 return headline, source
#             else: # If headline part is empty, treat the whole text as headline
#                 return text, ''
#     # If no pattern matches, return the original text as headline
#     return text, ''

class MMFakeBenchDataset(Dataset):
    def __init__(self, json_path: str, images_base_dir: str, limit: int = None):
        """
        Args:
            json_path: Path to MMFakeBench_test.json
            images_base_dir: Base dir where images are stored
            limit: Optional limit on number of samples
        """
        self.items = []
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if limit:
                data = data[:limit]

            for entry in data:
                image_path = os.path.join(images_base_dir, entry['image_path'].lstrip('/'))
                if os.path.exists(image_path):
                    self.items.append({
                        'image_path': image_path,
                        'text': entry['text'],
                        'label_binary': entry['gt_answers'],       # "True" or "Fake"
                        'label_multiclass': entry['fake_cls'],     # "original", "mismatch", etc.
                        'text_source': entry.get('text_source'),
                        'image_source': entry.get('image_source')
                    })

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        item = self.items[idx]
        return (
            item['image_path'],
            item['text'],
            item['label_binary'],
            item['label_multiclass'],
            item['text_source'],
            item['image_source']
        )

# ---------------------------------------------------------------------------
# Image Processor Module (No changes needed)
# ---------------------------------------------------------------------------
def encode_image(image_path: str) -> Tuple[Optional[str], Optional[str]]:
    """Encodes an image to base64, handling potential errors and converting formats."""
    try:
        # Check if the path exists and is a file
        if not os.path.isfile(image_path):
            logging.error(f"Image file not found or is not a file: {image_path}")
            return None, None

        with Image.open(image_path) as img:
            # Basic check for valid image format before extensive processing
            img.verify() # Verifies headers, doesn't load full image data

        # Re-open after verify
        with Image.open(image_path) as img:
            logging.debug(f"Processing image: {image_path} (Mode: {img.mode}, Format: {img.format})")
            # Convert formats like P, LA, RGBA with transparency to RGB
            if img.mode in ('RGBA', 'LA', 'P'):
                # If P mode with transparency, convert via RGBA
                if 'transparency' in img.info:
                     img = img.convert('RGBA')
                else:
                     img = img.convert('RGB')
            elif img.mode != 'RGB':
                 img = img.convert('RGB') # Convert other modes like L, CMYK etc. to RGB

            output_buffer = BytesIO()
            img.save(output_buffer, format='JPEG', quality=90) # Save as JPEG for consistency
            image_data = output_buffer.getvalue()

            # Double-check format after conversion/saving (optional but safe)
            # detected_format = imghdr.what(None, image_data)
            # if detected_format != 'jpeg':
                # logging.warning(f"Saved image format detected as {detected_format}, expected jpeg.")
                # # Force mime type if we are confident it should be jpeg
                # mime_type = "image/jpeg"
            # else:
            mime_type = "image/jpeg"

            base64_image = base64.b64encode(image_data).decode('utf-8')
            return base64_image, mime_type

    except FileNotFoundError:
        logging.error(f"Image file not found: {image_path}")
        return None, None
    except UnidentifiedImageError:
        logging.error(f"Cannot identify image file (possibly corrupt or unsupported format): {image_path}")
        return None, None
    except Exception as e:
        logging.error(f"Image processing failed for {image_path}: {str(e)}")
        return None, None

# =======================================================================
# Simple, Robust ImageHeadlineRelevancyChecker using ModelRouter
# =======================================================================
class ImageHeadlineRelevancyChecker:
    def __init__(self, model_router: ModelRouter, show_workflow: bool = False):
        self.model_router = model_router
        self.show_workflow = show_workflow
        self.system_prompt = (
            "You are a news fact-checking assistant. Analyze the relationship between a news image and headline.\n"
            "Determine if the image could reasonably illustrate the headline content. Consider:\n"
            "- People/objects shown vs described\n"
            "- Time indicators (clothing, technology, weather)\n"
            "- Location clues\n"
            "- Event specificity\n\n"
            'Respond ONLY in one of these two formats:\n'
            '"No, the image is appropriately used."\n'
            'OR\n'
            '"Yes, potential mismatch: [concise reason]."'
        )

    def check_relevancy(self, image_path: str, headline: str, max_retries: int = 3) -> Tuple[str, float, bool]:
        if not image_path or not os.path.isfile(image_path):
            logging.warning(f"Skipping relevancy check due to missing image file: {image_path}")
            return "Image File Not Found", 0.0, False
        if not headline or not headline.strip():
            logging.warning(f"Skipping relevancy check due to empty headline for image: {image_path}")
            return "Empty Headline Error", 0.0, False

        tries = 0
        while tries < max_retries:
            try:
                # Use centralized router for prompt, image handling, retries, provider quirks
                response = self.model_router.llm_multimodal(
                    self.system_prompt,
                    f"Headline: {headline}",
                    image_path
                )
                if not response:
                    raise RuntimeError("No response from model.")

                # Robustly get text content
                response_content = getattr(response, "content", None)
                if not response_content and isinstance(response, dict):
                    response_content = response.get("text")
                if not response_content:
                    raise RuntimeError("No content in model response.")
                response_content = response_content.strip()

                # Optionally confidence (you may keep your old confidence logic if models support it)
                confidence = 0.0
                if hasattr(response, 'response_metadata') and "logprobs" in getattr(response, "response_metadata", {}):
                    logprobs = response.response_metadata.get("logprobs", {}).get("content", None)
                    if logprobs:
                        logprob_values = [token["logprob"] for token in logprobs if "logprob" in token]
                        if logprob_values:
                            avg_logprob = sum(logprob_values) / len(logprob_values)
                            confidence = math.exp(avg_logprob)

                # Check format as before for final output
                if not ("No, the image is appropriately used." in response_content or
                        "Yes, potential mismatch:" in response_content):
                    logging.warning(f"Relevancy check got unexpected format: {response_content}")

                return response_content, confidence, True

            except Exception as e:
                msg = str(e).lower()
                logging.warning(f"Relevancy check LLM/API call failed (Try {tries+1}/{max_retries}): {e}")
                # Handle explicit image-format errors
                if ("invalid_image_format" in msg or "unsupported image" in msg or "image encoding" in msg):
                    return "Unsupported Image Format Error", 0.0, False
                if tries == max_retries - 1:
                    logging.error(f"Relevancy check failed after {max_retries} tries for {image_path}: {e}")
                    return "API Error", 0.0, False
                # Backoff before retry!
                time.sleep(2 ** tries)
                tries += 1

        return "API Error", 0.0, False

# class RephraserModule:
#     def __init__(self, original_question, enriched_claim=None, image_context=None):
#         self.original_question = original_question
#         self.enriched_claim = enriched_claim or ""
#         self.image_context = image_context or ""

#         # Templates to rephrase the original question
#         self.templates = [
#             "Is there any article that questions whether {claim_or_question}?",
#             "Could this image be an example of {claim_or_question}?",
#             "Are there examples of {condition} that look like this image?",
#             "Has this image been used in any articles related to {condition}?",
#             "In what context has this image been used to discuss {condition}?",
#             "Are there known cases where {condition} looks similar to the provided image?",
#             "Could the provided image be misinterpreted as a case of {condition}?"
#         ]

#     def extract_condition(self):
#         # Simple heuristic to extract condition from the question or claim
#         for keyword in ["hyperdontia", "mesiodens", "extra teeth", "supernumerary teeth"]:
#             if keyword in self.original_question.lower() or self.enriched_claim.lower():
#                 return keyword
#         return "the medical condition shown"

#     def generate_rephrased_questions(self, max_rephrases=2):
#         condition = self.extract_condition()
#         claim_or_question = self.original_question.rstrip("?")

#         rephrased = []
#         random.shuffle(self.templates)

#         for template in self.templates:
#             if len(rephrased) >= max_rephrases:
#                 break
#             try:
#                 q = template.format(
#                     claim_or_question=claim_or_question,
#                     condition=condition
#                 )
#                 rephrased.append(q)
#             except Exception as e:
#                 continue

#         return rephrased

class EvidenceTagger(BaseModel):
    question: str
    answer: str
    enriched_claim: str

    def run(self, llm) -> str:
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             """Classify how the following Q&A pair relates to the claim.

Possible labels:
- supports: The answer provides evidence supporting the claim
- refutes: The answer provides evidence against the claim
- background: The answer gives general context but doesn't support/refute
- irrelevant: The answer is unrelated or vague

Respond ONLY with one of the labels above."""),

            ("human", "Claim: {claim}\nQ: {question}\nA: {answer}")
        ])

        chain = LLMChain(llm=llm, prompt=prompt)
        try:
            response = chain.invoke({
                "claim": self.enriched_claim,
                "question": self.question,
                "answer": self.answer
            })

            tag = response.get('text', '').strip().lower()
            valid_tags = {"supports", "refutes", "background", "irrelevant"}
            return tag if tag in valid_tags else "background"

        except Exception as e:
            logging.warning(f"Evidence tagging failed: {e}")
            return "background"

class ClaimEnrichmentTool(BaseModel):
    image_path: str = Field(description="Path to the image file")
    headline: str = Field(description="News headline text")

    def run(self, model_router) -> Tuple[str, str, bool]:
        system_prompt_template = (
            "You are an expert in news analysis. Process the provided image and headline.\n"
            "First, restate the core claim made by the headline in a concise sentence.\n"
            "Second, describe the visual elements and context of the image in detail (people, objects, setting, actions, text visible). Avoid making assumptions beyond what is visible.\n"
            "Format your response EXACTLY as follows:\n"
            "RESTATED CLAIM: <restated claim>\n"
            "IMAGE CONTEXT: <detailed image context>"
        )

        if not self.image_path or not os.path.isfile(self.image_path):
            logging.warning(f"Image file missing: {self.image_path}")
            return "Image File Not Found", "", False
        if not self.headline or not self.headline.strip():
            logging.warning("Headline is empty.")
            return "Empty Headline", "", False

        try:
            response = model_router.llm_multimodal(
                system_prompt_template,
                f"Headline: {self.headline}",
                self.image_path
            )
            # Robustly get text content
            text = getattr(response, "content", None)
            if not text and isinstance(response, dict):
                text = response.get("text")
            if not text:
                raise RuntimeError("No response content from model.")
            text = text.strip()

            claim_match = re.search(r"RESTATED CLAIM:\s*(.*?)\s*IMAGE CONTEXT:", text, re.DOTALL | re.IGNORECASE)
            context_match = re.search(r"IMAGE CONTEXT:\s*(.*)", text, re.DOTALL | re.IGNORECASE)
            restated_claim = claim_match.group(1).strip() if claim_match else ""
            image_context = context_match.group(1).strip() if context_match else ""
            return restated_claim, image_context, True
        except Exception as e:
            logging.error(f"Claim enrichment failed: {str(e)}")
            return f"API Error: {str(e)}", "", False

UNHELPFUL_ANSWER_PATTERNS = [
    "not contain information",
    "no search results",
    "insufficient information",
    "not found in the provided context",
    "could not determine",
    "no data",
    "not available in the results",
    "cannot be answered from the given context",
    "does not specify",
    "does not mention",
    "is not mentioned",
    "are not mentioned",
    "information about this is not in the context",
    "this question cannot be answered",
    "search results do not contain",
    "i am sorry, but the provided context does not",
    "context does not provide this information",
    "search results do not mention",
    "search results do not specify",
    "provided document mentions but does not list", # Example from logs
    "provided context does not list", # Example from logs
    "provided text does not contain information", # Example from logs
    "provided context does not contain the name", # Example from logs
    "does not contain information about the specific false statement", # Example from logs
]

class QAGenerationTool(BaseModel):
    enriched_claim: str = Field(description="Enriched claim text")
    image_context: str = Field(description="Detailed image context")
    previous_qa: Optional[List[Dict[str, str]]] = Field(
        default=None, description="List of earlier Q-A pairs, including their answers"
    )

    def run(self, llm: Any) -> Tuple[str, bool]:
        """
        Generates a single, search-ready question designed to gather new evidence
        for the enriched_claim, considering previous Q&A attempts.

        Returns:
            A tuple containing the generated question (str) and a success flag (bool).
            If an error occurs, the "question" string might contain error details.
        """

        # ‚îÄ‚îÄ 1. Construct the System Prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # This prompt guides the LLM on how to generate the next question.
        # It now includes detailed instructions for handling previous Q&A.
        system_message_content = f"""
You are an expert fact-checker and a meticulous question generator.
Your primary objective is to generate ONE distinct, web-searchable question to gather new factual evidence for verifying the 'Statement' below. Each question should be a strategic step towards confirming or refuting parts of the Statement.

Statement to Verify:
```{self.enriched_claim}```

Contextual Image Details (may or may not be relevant for all questions):
```{self.image_context}```

You will be provided with 'Previous Q-A pairs'. Analyze them carefully to guide your NEW question:

1.  **If 'Previous Q-A pairs' exist:**
    *   **Avoid Redundancy:** Your NEW question MUST seek information NOT already clearly and factually provided by helpful previous answers. Do not ask for the same facts again if they have been successfully answered.
    *   **React to Unhelpful Answers (Critical):** An unhelpful answer is one explicitly marked with `[Unhelpful Answer]` or clearly indicates that the information could not be found (e.g., phrases like "not found," "no information," "cannot be answered").
        If a previous question resulted in such an uninformative or unhelpful answer:
        *   Your NEW question MUST explore a *significantly different aspect* of the Statement, or seek *related but distinct details*.
        *   Alternatively, if attempting to verify a similar detail, you MUST *substantially change the angle, specificity, or key entities* of your query (e.g., broaden if previously too narrow, narrow if too broad, use different keywords if an initial set yielded nothing).
        *   **CRITICALLY: DO NOT simply rephrase the failed question or ask for the exact same piece of information that was previously unanswerable. Your task is to find a *new path* to verifiable facts, not repeat dead ends.**
    *   **Build on Useful Answers:** If previous answers were informative and factual, your NEW question can aim to:
        a) Uncover *new, complementary information* not yet explored.
        b) Delve deeper into a relevant detail that was mentioned but not fully elaborated.
        c) Seek corroboration for a critical fact from a different perspective if necessary (use this sparingly).

2.  **General Rules for ALL questions (whether previous Q-A exists or not):**
    *   Identify a *new* concrete, verifiable information gap related to the 'Statement' (e.g., specific entities, people, organizations, locations, dates, actions, quantities, or outcomes directly related to the Statement that have not yet been adequately covered or clarified by prior helpful Q&A).
    *   Form ONE neutral, concise, and web-searchable question that aims to fill that specific gap with a factual answer.
    *   The question should be answerable with facts, not subjective opinions, speculations, or interpretations.
    *   Avoid asking for the process of verification, the existence of sources, videos, calculations, or detailed methodology (the WebQA module will attempt to find sources for factual questions).
    *   Never use the words ‚Äúclaim‚Äù or "statement" in your generated question; instead, refer to the specific elements from the 'Statement to Verify' that your question targets.

Return your output strictly and only in the following format:
QUESTION: <your carefully crafted question>
"""

        # ‚îÄ‚îÄ 2. Prepare `previous_qa_formatted` for the Human Message ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if self.previous_qa:
            prev_qa_list_str = []
            for i, qa_pair in enumerate(self.previous_qa):
                question_text = qa_pair.get('question', 'N/A')
                answer_text = qa_pair.get('answer', 'N/A')
                # Check if the answer from the previous QA pair was unhelpful
                answer_is_unhelpful = any(pattern in answer_text.lower() for pattern in UNHELPFUL_ANSWER_PATTERNS)
                indicator = " [Unhelpful Answer]" if answer_is_unhelpful else ""
                prev_qa_list_str.append(f"Q{i+1}: {question_text}\nA{i+1}: {answer_text}{indicator}")

            if prev_qa_list_str:
                previous_qa_formatted = "Previous Q-A pairs (analyze carefully, especially unhelpful ones, to generate a NEW and DIFFERENT question):\n" + "\n---\n".join(prev_qa_list_str)
            else: # Should not happen if self.previous_qa is not empty, but a safeguard
                previous_qa_formatted = "No previous Q-A pairs to display, but previous Q-A pairs were provided (possibly empty)."
        else:
            previous_qa_formatted = "No previous Q-A pairs. This is the first question for this line of inquiry."


        # ‚îÄ‚îÄ 3. Create Prompt Template and Inputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system", system_message_content),
                ("human", "{previous_qa_formatted_for_human_message}"),
            ]
        )

        inputs = {
            "previous_qa_formatted_for_human_message": previous_qa_formatted,
        }

        # ‚îÄ‚îÄ 4. LLM Call ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            chain = LLMChain(llm=llm, prompt=prompt_template)
            response = chain.invoke(inputs)

            raw_text = ""
            if isinstance(response, dict):
                raw_text = response.get('text', "").strip()
            elif isinstance(response, str): # Some LLMs might return a string directly
                raw_text = response.strip()
            else: # Handle Langchain's AIMessage or other message objects
                if hasattr(response, 'content'):
                    raw_text = str(response.content).strip()
                else:
                    raw_text = str(response).strip()


            # ‚îÄ‚îÄ 5. Parse ‚ÄúQUESTION:‚Äù line ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            match = re.search(r"QUESTION:\s*(.+)", raw_text, re.IGNORECASE | re.DOTALL)
            generated_question = match.group(1).strip() if match else raw_text.strip()

            if not generated_question or generated_question.lower() == "question:":
                logging.error(f"QAGenerationTool produced an empty or malformed question. Raw response: '{raw_text}' from inputs: {inputs}")
                return "Error: Empty question generated", False

            # Optional: More sophisticated check for actual repetition beyond simple string match
            # For now, the prompt is the main guard against repetition.
            if self.previous_qa:
                for prev_q_pair in self.previous_qa:
                    if prev_q_pair.get('question', '').strip().lower() == generated_question.strip().lower():
                        logging.warning(
                            f"QAGenerationTool generated a question very similar or identical to a previous one: '{generated_question}'. "
                            f"This may indicate the LLM is not fully adhering to the 'avoid repetition' instruction for unhelpful answers."
                        )
                        # Decide if this should be a hard fail or just a warning. For now, warning.
                        break


            logging.info(f"QAGenerationTool successfully generated question: '{generated_question}'")
            return generated_question, True

        except Exception as exc:
            logging.error(f"QAGenerationTool LLM call or processing failed: {str(exc)}", exc_info=True)
            return f"API Error in QAGenerationTool: {str(exc)}", False

# # Step 2a: Q-A Generation Tool
# class QAGenerationTool(BaseModel):
#     enriched_claim: str = Field(description="Enriched claim text")
#     image_context: str = Field(description="Detailed image context")
#     previous_qa: Optional[List[Dict[str, str]]] = Field(default=None, description="List of previous Q-A pairs")

#     def run(self, llm) -> Tuple[str, bool]: # Return success flag
#         prompt_template = ChatPromptTemplate.from_messages([
#             ("system",
#              """You are a question generator for fact-checking. Based on the claim and image context, generate ONE specific, verifiable question to help determine the truthfulness of the claim. The question should be answerable via web search.
# If previous Q-A pairs are provided, generate a NEW question that explores a different aspect or asks for more specific details not covered yet.
# Focus on concrete details (names, locations, dates, events, specific numbers mentioned or visible). Avoid overly broad or subjective questions.
# Format your response EXACTLY as:
# QUESTION: <your question>"""),
#             ("human", "Claim: {enriched_claim}\nImage Context: {image_context}\n{previous_qa_formatted}")
#         ])
#         # Use LLMChain for this structured generation
#         chain = LLMChain(llm=llm, prompt=prompt_template)
#         previous_qa_str = ""
#         if self.previous_qa:
#             previous_qa_str = "Previous Q-A pairs:\n" + "\n".join([f"Q: {qa['question']} | A: {qa['answer']}" for qa in self.previous_qa])

#         input_data = {
#             "enriched_claim": self.enriched_claim,
#             "image_context": self.image_context,
#             "previous_qa_formatted": previous_qa_str
#         }

#         try:
#             # Use .invoke for Chains in newer Langchain versions
#             response = chain.invoke(input_data)
#             # LLMChain output is often in a dict under 'text' key
#             question_response = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()

#             question = ""
#             # Use regex for robustness
#             match = re.search(r"QUESTION:\s*(.*)", question_response, re.IGNORECASE)
#             if match:
#                 question = match.group(1).strip()
#             else:
#                 # If format fails, maybe the whole response is the question? Use with caution.
#                 logging.warning(f"QAGenTool failed to parse 'QUESTION:' format. Response: {question_response}")
#                 question = question_response # Take the whole response as question? Or return error?

#             if not question:
#                  logging.error(f"QAGenTool generated an empty question. Response: {question_response}")
#                  return "", False # Failed to generate a question

#             return question, True
#         except Exception as e:
#             logging.error(f"Error during QA generation LLM call: {str(e)}")
#             return f"API Error: {str(e)}", False

class WebQAModule(BaseModel):
    question: str = Field(description="Question to answer using web search")
    model_router: Any = Field(description="Model router instance")
    max_context_length: int = Field(default=15000)
    search_result_count: int = Field(default=3) # Reduced default

    def _search_duckduckgo(self) -> Tuple[Optional[List[Dict]], Optional[Exception]]:
        """
        Performs a DuckDuckGo search with enhanced retry logic and error handling.
        Returns a list of search result dictionaries or None, and an exception if all retries fail.
        """
        MAX_DDG_ATTEMPTS = 3  # Total number of attempts for a single search call
        INITIAL_SLEEP_TIME_SECONDS = 10
        DDG_TIMEOUT_SECONDS = 20 # Timeout for each individual ddgs.text() call

        search_results_list: Optional[List[Dict]] = None
        last_exception: Optional[Exception] = None

        for attempt in range(MAX_DDG_ATTEMPTS):
            try:
                logging.info(f"[DuckDuckGo] Search attempt {attempt + 1}/{MAX_DDG_ATTEMPTS} for: \"{self.question}\" with timeout {DDG_TIMEOUT_SECONDS}s.")

                # DDGS() context manager handles underlying session management.
                with DDGS(timeout=DDG_TIMEOUT_SECONDS) as ddgs: # Set timeout on the DDGS session object
                    # ddgs.text() itself can be blocking. The session timeout helps.
                    # For more granular control, one might need to wrap ddgs.text in a thread with timeout if DDGS lib doesn't respect it well for all operations.
                    raw_results = ddgs.text(self.question, max_results=self.search_result_count) # Removed explicit timeout here if session timeout is effective

                    # Ensure raw_results is consumed and checked
                    current_results = [r for r in raw_results]

                search_results_list = current_results
                logging.info(f"[DuckDuckGo] Attempt {attempt + 1} successful, found {len(search_results_list)} results.")
                return search_results_list, None  # Success, return results and no error

            except Exception as e:
                last_exception = e
                logging.warning(f"[DuckDuckGo] Error on search attempt {attempt + 1}/{MAX_DDG_ATTEMPTS}: {type(e).__name__} - {e}")

                # Check for specific rate limit or ignorable errors if possible from exception details
                # For now, generic retry for any exception during DDG search.

                if attempt < MAX_DDG_ATTEMPTS - 1:
                    # Exponential backoff: e.g., 10s, 20s for subsequent retries
                    sleep_time = INITIAL_SLEEP_TIME_SECONDS * (2 ** attempt)
                    logging.info(f"[DuckDuckGo] Sleeping for {sleep_time}s before next attempt...")
                    time.sleep(sleep_time)
                else:
                    logging.error(f"[DuckDuckGo] All {MAX_DDG_ATTEMPTS} search attempts failed. Last error: {e}")
                    return None, last_exception # Return None for results and the last exception

        # Fallback, should ideally be covered by the loop's return statements
        return None, last_exception if last_exception else Exception("DuckDuckGo search loop completed unexpectedly.")

    def run(self) -> Tuple[str, bool]:
        if not self.question or self.question.strip() == "":
            logging.warning("Skipping WebQA: Question is empty.")
            return "Skipped due to empty question", False

        final_search_results_list, search_error = self._search_duckduckgo()

        if search_error and final_search_results_list is None: # Persistent search failure
            error_msg = f"WebQA Error: DuckDuckGo search failed after multiple attempts. Last error: {search_error}"
            logging.error(error_msg)
            return error_msg, False

        if not final_search_results_list: # Search successful but no items found, or list is None without error (should be rare)
            logging.warning(f"[DuckDuckGo] Returned no results for: {self.question}")
            context_str = "No search results found."
        else:
            logging.info(f"[DuckDuckGo] Successfully fetched {len(final_search_results_list)} results.")
            # Ensure 'body' exists and is a string before joining
            context_str = "\n".join([res.get('body', '') for res in final_search_results_list if isinstance(res.get('body'), str)])
            if not context_str.strip(): # Check if all snippets were empty or non-string
                 logging.warning(f"[DuckDuckGo] Search results were found, but all snippets were empty or non-string for: {self.question}")
                 context_str = "No meaningful search results found (empty or invalid snippets)."
            else:
                 logging.info(f"[DuckDuckGo] Raw context for \"{self.question}\": {context_str[:200]}...")
                 logging.info(f"Got search results text (length: {len(context_str)}).")
                 if len(context_str) > self.max_context_length:
                    context_str = context_str[:self.max_context_length] + "... (truncated)"
                    logging.info(f"Search results context truncated to {self.max_context_length} characters.")

        # --- Synthesize Answer using LLM ---
        synthesis_prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are a fact-checking assistant. Based ONLY on the provided search results context, answer the following question concisely and accurately. If the results do not contain sufficient information to answer the question, state that the information is not found in the provided context. Do not add information not present in the results. Do not make assumptions."),
            ("human", "Search Results Context:\n---\n{context_str}\n---\nQuestion: {question}\nAnswer:")
        ])

        try:
            chain = LLMChain(llm=self.model_router.get_model(), prompt=synthesis_prompt_template)
            input_variables = {
                "context_str": context_str,
                "question": self.question
            }
            response = chain.invoke(input_variables)
            answer = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()

            if not answer:
                logging.warning("WebQA synthesis LLM returned an empty answer.")
                answer = "Could not determine a conclusive answer from the provided search results context."

            logging.info(f"Synthesized answer for \"{self.question}\": \"{answer[:100]}...\"")
            return answer, True

        except Exception as e:
            logging.error(f"Error during WebQA answer synthesis: {type(e).__name__} - {str(e)}")
            return f"WebQA Error: Synthesis failed ({type(e).__name__} - {str(e)})", False

# import unittest
# from unittest.mock import MagicMock, patch
# # from your_module_name import WebQAModule  # Replace with actual module name

# class TestWebQAModule(unittest.TestCase):

#     def setUp(self):
#         # Example question and API key for testing
#         self.question = "What is quantum computing?"
#         self.brave_api_key = "test_brave_api_key"
#         self.model_router_mock = MagicMock()

#     @patch('your_module_name.BraveSearchWrapper')  # Replace with correct path
#     def test_run_successful_search_and_answer(self, mock_brave_wrapper):
#         """Test that run() successfully performs search and generates answer."""
#         # Arrange
#         mock_instance = mock_brave_wrapper.return_value
#         mock_instance.run.return_value = "Quantum computing is a type of computation..."

#         module = WebQAModule(
#             question=self.question,
#             brave_api_key=self.brave_api_key,
#             model_router=self.model_router_mock
#         )

#         # Mock LLM response from model_router.get_model().invoke()
#         llm_response = MagicMock()
#         llm_response.content = "The answer is: Quantum computing leverages qubits..."
#         module.model_router.get_model().invoke.return_value = llm_response

#         # Act
#         result, success = module.run()

#         # Assert
#         self.assertTrue(success)
#         self.assertIn("qubits", result.lower())
#         mock_brave_wrapper.assert_called_once_with(api_key=self.brave_api_key, k=5)
#         module.model_router.get_model().invoke.assert_called()

#     def test_run_empty_question_returns_skipped(self):
#         """Test that empty question leads to skipped status."""
#         module = WebQAModule(
#             question="",
#             brave_api_key=self.brave_api_key,
#             model_router=self.model_router_mock
#         )
#         result, success = module.run()
#         self.assertEqual(result, "Skipped due to empty question")
#         self.assertFalse(success)

#     @patch('your_module_name.BraveSearchWrapper')
#     def test_run_brave_search_returns_no_results(self, mock_brave_wrapper):
#         """Test case where Brave returns no results."""
#         mock_instance = mock_brave_wrapper.return_value
#         mock_instance.run.return_value = ""

#         module = WebQAModule(
#             question=self.question,
#             brave_api_key=self.brave_api_key,
#             model_router=self.model_router_mock
#         )

#         result, success = module.run()

#         self.assertIn("No search results found.", result)
#         self.assertTrue(success)  # Still considered successful, just no info

#     @patch('your_module_name.BraveSearchWrapper')
#     def test_run_llm_throws_exception(self, mock_brave_wrapper):
#         """Test that exceptions during LLM call are caught properly."""
#         mock_instance = mock_brave_wrapper.return_value
#         mock_instance.run.return_value = "Some search context"

#         module = WebQAModule(
#             question=self.question,
#             brave_api_key=self.brave_api_key,
#             model_router=self.model_router_mock
#         )

#         # Simulate LLM throwing an error
#         module.model_router.get_model().invoke.side_effect = Exception("LLM Error")

#         result, success = module.run()

#         self.assertIn("Error during WebQA module", result)
#         self.assertFalse(success)

#     @patch('your_module_name.BraveSearchWrapper')
#     def test_run_brave_raises_error(self, mock_brave_wrapper):
#         """Test case where BraveSearchWrapper raises an exception."""
#         mock_brave_wrapper.side_effect = Exception("Brave API failed")

#         module = WebQAModule(
#             question=self.question,
#             brave_api_key=self.brave_api_key,
#             model_router=self.model_router_mock
#         )

#         result, success = module.run()

#         self.assertIn("WebQA Error", result)
#         self.assertFalse(success)

def score_qa_evidence_support(qa_pairs: List[Dict[str, str]], claim: str) -> Tuple[int, int, int]:
    """
    Scores QA pairs for relevance:
    - Directly supporting/refuting claim
    - General background
    Returns: (direct_support, general_background, total)
    """
    direct = 0
    background = 0

    for qa in qa_pairs:
        answer = qa["answer"].lower()
        claim_keywords = claim.lower().split()

        # Heuristic: check if claim terms appear in the answer
        if any(kw in answer for kw in claim_keywords[:5]):
            direct += 1
        elif len(answer.strip()) > 30:
            background += 1

    return direct, background, len(qa_pairs)

def evidence_strength_score(qa_pairs: List[Dict[str, str]], claim: str) -> float:
    """
    Assigns a score based on how many QAs directly relate to the claim.
    Output: float score between 0 and 1
    """
    if not qa_pairs:
        return 0.0
    direct, _, total = score_qa_evidence_support(qa_pairs, claim)
    return direct / total

# Step 3: QA Selection Tool (Not used if num_claims_per_branch=1)
class QASelectionTool(BaseModel):
    qa_pairs: List[Dict[str, str]] = Field(description="List of Q-A pairs from a single branch")

    def run(self, llm) -> Tuple[Optional[Dict[str, str]], bool]: # Return success flag
        if not self.qa_pairs:
            return None, True # Nothing to select
        if len(self.qa_pairs) == 1:
            return self.qa_pairs[0], True # Only one pair, return it

        prompt = ChatPromptTemplate.from_messages([
            ("system",
            """You are an expert fact-checker. You are given several question-answer pairs related to a claim.
                Your task is to select the most useful pair for verifying the claim.

                Selection Criteria:
                - Most relevant to evaluating the claim.
                - Most informative and precise.
                - Avoid vague or inconclusive answers.

                Respond only with the selected pair in the format:
                Question: <question>
                Answer: <answer>
                """),
            ("human", "Available Q-A Pairs:\n{qa_pairs_formatted}")
        ])
        chain = LLMChain(llm=llm, prompt=prompt)

        qa_formatted = "\n---\n".join([f"Pair {i+1}:\nQ: {qa['question']}\nA: {qa['answer']}" for i, qa in enumerate(self.qa_pairs)])
        input_data = {"qa_pairs_formatted": qa_formatted}

        try:
            response = chain.invoke(input_data)
            selection_response = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()

            # Robust parsing using regex
            match = re.search(r"Question:\s*(.*?)\s*Answer:\s*(.*)", selection_response, re.DOTALL | re.IGNORECASE)

            if match:
                selected_question = match.group(1).strip()
                selected_answer = match.group(2).strip()
                # Find the original dict to return
                for qa in self.qa_pairs:
                    if qa['question'] == selected_question: # Simple match, might need fuzzy matching if LLM slightly reformats
                       return qa, True
                # If exact match fails, maybe return the parsed strings?
                logging.warning("QASelectionTool could not find exact match for selected pair, returning parsed strings.")
                return {"question": selected_question, "answer": selected_answer}, True

            else:
                logging.error(f"QASelectionTool failed to parse SELECTED PAIR format. Response: {selection_response}")
                # Fallback: return the first pair? or None?
                return self.qa_pairs[0], False # Indicate parsing failure

        except Exception as e:
            logging.error(f"Error during QA selection LLM call: {str(e)}")
            return None, False

class VeracityClassifier(BaseModel):
    """
    Classifies the truthfulness of an enriched claim given its image context and associated Q&A pairs.
    Uses a specified LLM (OpenAI or Gemini).
    Returns: (label, reason, success)
    """
    enriched_claim: str
    image_context: str
    qa_pairs: List[Dict[str, str]]

    def run(self, llm) -> Tuple[str, str, bool]:
        prompt = ChatPromptTemplate.from_messages([
            ("system",
            """You are a misinformation expert evaluating the truthfulness of the following enriched claim.

Claim: "{enriched_claim}"

Image Context:
{image_context}

Evidence gathered from Q&A pairs:
{qa_pairs_formatted}

Your task is to evaluate explicitly if the ORIGINAL CLAIM described is true or false, based strictly on the provided evidence.

Use ONLY one of these labels:
- Claim Supported (the original claim described is TRUE, supported by evidence)
- Claim Refuted (the original claim described is FALSE, evidence explicitly contradicts it)
- Dispute Supported (the claim explicitly disputes or denies a misinformation claim and evidence SUPPORTS that dispute, indicating the original claim is FALSE)
- Uncertain (insufficient evidence to confidently verify or refute the claim)

Provide your evaluation EXACTLY as follows:

VERACITY: <one of the specified labels>
REASON: <your complete, detailed reasoning based on provided evidence>

Example Response:
VERACITY: Claim Refuted
REASON: The evidence from multiple Q&A pairs shows that the claim is contradicted by historical records...
""")
        ])
        qa_formatted = "\n\n".join([f"Q: {qa['question']}\nA: {qa['answer']}" for qa in self.qa_pairs])
        input_data = {
            "enriched_claim": self.enriched_claim or "(enrichment missing)",
            "image_context": self.image_context or "(image context missing)",
            "qa_pairs_formatted": qa_formatted or "(no Q&A pairs found)",
        }

        try:
            # -- Special handling for Gemini (must use message list interface, all prompt in SystemMessage) --
            if hasattr(llm, "model") and "gemini" in llm.model.lower():
                system_prompt_text = prompt.format(**input_data)
                messages = [
                    SystemMessage(content=system_prompt_text),
                    HumanMessage(content="")  # Empty or very short user message is best!
                ]
                response = llm.invoke(messages)
                result_text = response.content.strip() if hasattr(response, "content") else str(response)
            else:
                # For OpenAI and others, use LLMChain as usual
                chain = LLMChain(llm=llm, prompt=prompt)
                response = chain.invoke(input_data)
                result_text = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()

            # --- Parse the output ---
            veracity_match = re.search(r"VERACITY:\s*([A-Za-z ]+)", result_text, re.IGNORECASE)
            reason_match = re.search(r"REASON:\s*(.+)", result_text, re.DOTALL | re.IGNORECASE)
            veracity = "Uncertain"
            reason = ""
            if veracity_match:
                veracity_raw = veracity_match.group(1).strip()
                # Accept your label variants. Expand if needed.
                allowed_labels = {
                    "supported": "Supported",
                    "refuted": "Refuted",
                    "uncertain": "Uncertain",
                    "claim supported": "Claim Supported",
                    "claim refuted": "Claim Refuted",
                    "dispute supported": "Dispute Supported"
                }
                for norm, label in allowed_labels.items():
                    if veracity_raw.lower() == norm:
                        veracity = label
                        break
                else:
                    veracity = veracity_raw  # fallback to whatever the LLM returned
            if reason_match:
                reason = reason_match.group(1).strip()
            if not veracity_match or not reason_match:
                logging.warning(f"VeracityClassifier failed to parse format. Response: {result_text}")
                return veracity, reason, True
            return veracity, reason, True

        except Exception as e:
            logging.error(f"Error during VeracityClassifier LLM call: {str(e)}")
            return "Uncertain", f"API Error: {str(e)}", False

from langchain_core.messages import SystemMessage, HumanMessage

class FinalClassifier(BaseModel):
    relevancy_response: str
    relevancy_confidence: float
    enriched_claim: str
    image_context: str
    qa_pairs: List[Dict[str, str]]
    preliminary_veracity: str
    preliminary_reason: str

    def run(self, llm) -> Tuple[str, str, bool]:
        """
        Synthesizes evidence and makes a final misinformation classification.
        Returns (decision_label, explanation, success_flag).
        Only "Misinformation" or "Not Misinformation" or "Uncertain" are valid outputs.
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
                You are a senior misinformation detection expert. Synthesize all provided evidence carefully and critically.

                **Evidence Summary:**
                1. **Image-Headline Relevancy Check:**
                    * Assessment: {relevancy_response}
                    * Confidence: {relevancy_confidence:.2f}

                2. **Claim & Image Context:**
                    * Claim: {enriched_claim}
                    * Image Context: {image_context}

                3. **Fact-Checking Q&A:**
                {qa_pairs_formatted}

                4. **Preliminary Veracity Assessment:**
                    * Assessment: {preliminary_veracity}
                    * Reason: {preliminary_reason}

                ---

                **IMPORTANT ‚Äì How to interpret Preliminary Veracity Labels:**
                - "Claim Supported" means the ORIGINAL CLAIM is TRUE. (Therefore, NOT misinformation.)
                - "Claim Refuted" or "Dispute Supported" means the ORIGINAL CLAIM is FALSE. (Therefore, it IS misinformation.)
                - "Uncertain" means evidence is inconclusive, so make your best judgment based on all available evidence.

                ---

                **Instructions for your Final Decision:**

                - Critically evaluate if the original claim is misinformation, referencing the evidence above.
                - Output your judgment using EXACTLY one of the following options, and nothing else:
                  - FINAL DECISION: Misinformation


                - After your decision, always provide a detailed, step-by-step rationale referencing the evidence.

                **Format:**
                FINAL DECISION: [Misinformation/Not Misinformation/Uncertain]
                EXPLANATION: [detailed rationale]
                """)
        ])

        qa_formatted = "\n   ".join([f"Q: {qa['question']}\n   A: {qa['answer']}" for qa in self.qa_pairs]) or "(No Q&A pairs generated)"

        formatted_prompt = prompt.format(
            relevancy_response=self.relevancy_response,
            relevancy_confidence=self.relevancy_confidence,
            enriched_claim=self.enriched_claim or "(Claim enrichment failed)",
            image_context=self.image_context or "(Image context extraction failed)",
            qa_pairs_formatted=qa_formatted,
            preliminary_veracity=self.preliminary_veracity,
            preliminary_reason=self.preliminary_reason
        )

        try:
            # Use correct message format for Gemini or OpenAI
            if "gemini" in llm.model.lower():
                messages = [
                    SystemMessage(content="You are a senior misinformation detection expert. Synthesize all provided evidence to make a final judgment."),
                    HumanMessage(content=formatted_prompt)
                ]
                response = llm.invoke(messages)
                if hasattr(response, "content"):
                    result = response.content.strip()
                else:
                    result = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()
            else:
                chain = LLMChain(llm=llm, prompt=prompt)
                response = chain.invoke({
                    "relevancy_response": self.relevancy_response,
                    "relevancy_confidence": self.relevancy_confidence,
                    "enriched_claim": self.enriched_claim or "(Claim enrichment failed)",
                    "image_context": self.image_context or "(Image context extraction failed)",
                    "qa_pairs_formatted": qa_formatted,
                    "preliminary_veracity": self.preliminary_veracity,
                    "preliminary_reason": self.preliminary_reason,
                })
                result = response.get('text', '').strip() if isinstance(response, dict) else str(response).strip()

            # Parse decision and explanation (allow only the two options)
            decision_match = re.search(
                r"FINAL DECISION:\s*(Misinformation|Not Misinformation|Uncertain)",
                result, re.IGNORECASE
            )
            explanation_match = re.search(r"EXPLANATION:\s*(.*)", result, re.DOTALL | re.IGNORECASE)

            decision = "Error"
            explanation = ""

            if decision_match:
                decision_raw = decision_match.group(1).strip().lower()
                if decision_raw == "misinformation":
                    decision = "Misinformation"
                elif decision_raw == "not misinformation":
                    decision = "Not Misinformation"
            # Any ambiguous result (e.g. uncertain/format error) will yield "Error"
            if explanation_match:
                explanation = explanation_match.group(1).strip()

            refusal_patterns = ["sorry", "cannot", "unable to proceed", "cannot fulfill this request", "not sure"]
            if any(pattern in result.lower() for pattern in refusal_patterns) and decision == "Error":
                logging.warning(f"FinalClassifier returned a refusal or uncertain answer: {result}")
                decision = "Model Refusal"
                explanation = result
            elif decision == "Error" or not explanation:
                logging.warning(f"FinalClassifier failed to parse format. Response: {result}")
                decision = "Format Error"
                explanation = result

            return decision, explanation, decision in ("Misinformation", "Not Misinformation", "Uncertain")

        except Exception as e:
            logging.error(f"Error during FinalClassifier LLM call: {str(e)}")
            return "API Error", f"API Error: {str(e)}", False

# =======================================================================
# Modified AdvancedRAGPipeline to use ModelRouter
# =======================================================================
class AdvancedRAGPipeline:
    def __init__(self, model_router: ModelRouter, brave_api_key: str, verbose: bool = False):
        self.model_router = model_router
        self.brave_api_key = brave_api_key
        self.verbose = verbose
        self.relevancy_checker = ImageHeadlineRelevancyChecker(model_router=model_router)
        logging.info(f"AdvancedRAGPipeline initialized with model: {model_router.model_name}")

    def run_pipeline(self, image_path: str, headline: str,
                     num_claims_per_branch: int = 1, num_chains: int = 2) -> Dict[str, Any]:
        """Runs the full misinformation detection pipeline for a single image-headline pair."""
        results = {
            "status": "Processing",
            "image_path": image_path,
            "headline": headline,
            "relevancy_result": None,
            "relevancy_confidence": 0.0,
            "enriched_claim": None,
            "image_context": None,
            "qa_pairs": [],
            "preliminary_veracity": "N/A",
            "preliminary_reason": "N/A",
            "final_decision": "Pipeline Error",
            "explanation": "Pipeline did not complete.",
            "error_details": None,
            "model_used": self.model_router.model_name  # Track which model was used
        }

        timers = 5
        try:
            # --- Step 0: Relevancy Check ---
            if self.verbose: logging.info("Step 0: Running Relevancy Check...")
            relevancy_response, relevancy_confidence, relevancy_success = self.relevancy_checker.check_relevancy(image_path, headline)
            results["relevancy_result"] = relevancy_response
            results["relevancy_confidence"] = relevancy_confidence
            if not relevancy_success:
                if "Image Encoding Error" in relevancy_response or "Unsupported Image Format Error" in relevancy_response:
                     results["status"] = "Failed - Image Error"
                elif "Empty Headline Error" in relevancy_response:
                     results["status"] = "Failed - Empty Headline"
                else:
                     results["status"] = "Failed - Relevancy Check Error"
                results["error_details"] = relevancy_response
                results["final_decision"] = results["status"]
                results["explanation"] = f"Pipeline stopped at relevancy check. Reason: {relevancy_response}"
                logging.error(f"{results['status']} for {image_path}. Reason: {relevancy_response}")
                return results
            if self.verbose:
                 logging.info(f"Relevancy: {relevancy_response} (Conf: {relevancy_confidence:.2f}")

             # --- Step 1: Enrich Claim ---
            if self.verbose: logging.info("Step 1: Running Claim Enrichment...")
            enrich_tool = ClaimEnrichmentTool(image_path=image_path, headline=headline)
            enriched_claim, image_context, enrich_success = enrich_tool.run(self.model_router)
            results["enriched_claim"] = enriched_claim
            results["image_context"] = image_context
            if not enrich_success:
                 results["status"] = "Failed - Enrichment Error"
                 results["error_details"] = enriched_claim
                 results["final_decision"] = results["status"]
                 results["explanation"] = f"Pipeline stopped at claim enrichment. Reason: {enriched_claim}"
                 logging.error(f"{results['status']} for {image_path}. Reason: {enriched_claim}")
                 return results

            if self.verbose:
                 logging.info(f"Enriched Claim: {enriched_claim}")
                 logging.info(f"Image Context: {image_context}")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Step 2 ‚Ä¢ Iterative Q-A generation and answering  (no Rephraser)
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if self.verbose:
                logging.info(
                    f"Step 2: Running Q&A Generation ({num_chains} chains, "
                    f"{num_claims_per_branch} Qs/chain)‚Ä¶"
                )

            global_qa_pairs: list[dict] = []
            UNHELPFUL = [
                "not contain information",
                "no search results",
                "insufficient information",
                "not found in the provided context",
                "could not determine",
                "no data",
                "not available in the results",
            ]

            for chain_idx in range(num_chains):
                if self.verbose:
                    logging.info(f"--- Q&A Chain {chain_idx + 1} ---")
                qa_branch: list[dict] = []

                for i in range(num_claims_per_branch):
                    retries = 0
                    MAX_RETRIES_PER_QUESTION = 3
                    while retries < MAX_RETRIES_PER_QUESTION:
                        # 2-1 ‚ñ∏ generate a fresh question
                        q_tool = QAGenerationTool(
                            enriched_claim=enriched_claim,
                            image_context=image_context,
                            previous_qa=qa_branch + global_qa_pairs,
                        )
                        question, ok = q_tool.run(self.model_router.get_model())
                        time.sleep(timers)
                        if not ok or not question:
                            retries += 1
                            logging.warning(f"QAGenerationTool failed or returned empty. Attempt {retries}/{MAX_RETRIES_PER_QUESTION}")
                            continue
                        if self.verbose:
                            logging.info(f"Generated Q{chain_idx+1}.{i+1}: {question}")

                        # 2-2 ‚ñ∏ try Web-QA
                        qa_mod = WebQAModule(
                            question=question,
                            model_router=self.model_router,
                        )

                        answer, ok_webqa = qa_mod.run() # Use a different variable for success flag
                        time.sleep(timers) # Existing sleep after WebQA call

                        # Check if WebQA was successful AND the answer is helpful
                        if ok_webqa and not any(p in answer.lower() for p in UNHELPFUL):
                            # Successful and helpful answer
                            break # Break from the while loop (MAX_RETRIES_PER_QUESTION)
                        else:
                            # WebQA failed, or answer was unhelpful
                            retries += 1
                            logging.warning(f"WebQA failed (ok_webqa={ok_webqa}) or answer unhelpful for Q: '{question}'. Answer: '{answer[:100]}...'. Attempt {retries}/{MAX_RETRIES_PER_QUESTION} for this question slot.")
                            # The loop will continue, QAGenerationTool will generate a new question
                            # (as previous_qa now includes the failed/unhelpful attempt)
                            # Or, if retries exhausted, this question slot in the branch will be marked as failed.
                            if retries >= MAX_RETRIES_PER_QUESTION:
                                logging.error(f"Exhausted retries for generating a useful Q&A for chain {chain_idx+1}, question slot {i+1}.")
                                # The 'answer' will be the last one (likely an error or "not found"), and 'question' the last attempted.
                                # This will be stored as is.

                    # store result (even if final attempt failed after MAX_RETRIES_PER_QUESTION)
                    if not ok_webqa: # If WebQA ultimately failed for the last generated question
                        qa_branch.append(
                            {"question": question or f"Error generating Q{i+1}", "answer": answer or "N/A - WebQA failed"}
                        )
                    else: # WebQA was successful for the last question (or an earlier one in the retry loop)
                        tagger = EvidenceTagger(
                            question=question, answer=answer, enriched_claim=enriched_claim
                        )
                        ev_type = tagger.run(self.model_router.get_model())
                        qa_branch.append(
                            {"question": question, "answer": answer, "evidence_type": ev_type}
                        )
                    #     answer, ok = qa_mod.run()
                    #     logging.info(f"WebQA raw answer for Q{chain_idx+1}.{i+1}: {answer}")
                    #     time.sleep(timers)
                    #     if (not ok) or any(p in answer.lower() for p in UNHELPFUL):
                    #         # 2-2-b ‚ñ∏ ask the generator for a new question (avoid repeats)
                    #         retries += 1
                    #         prev = qa_branch + global_qa_pairs + [{"question": question, "answer": answer}]
                    #         alt_tool = QAGenerationTool(
                    #             enriched_claim=enriched_claim,
                    #             image_context=image_context,
                    #             previous_qa=prev,
                    #         )
                    #         question, ok = alt_tool.run(self.model_router.get_model())
                    #         time.sleep(timers)
                    #         if not ok or not question:
                    #             continue  # outer while-loop
                    #         qa_mod = WebQAModule(
                    #             question=question,
                    #             brave_api_key=self.brave_api_key,
                    #             model_router=self.model_router,
                    #         )
                    #         answer, ok = qa_mod.run()
                    #         logging.info(f"WebQA retry answer: {answer}")
                    #         time.sleep(timers)
                    #         if (not ok) or any(p in answer.lower() for p in UNHELPFUL):
                    #             continue  # outer while-loop

                    #     # 2-3 ‚ñ∏ success
                    #     break

                    # # store result (even if final attempt failed)
                    # if not ok:
                    #     qa_branch.append(
                    #         {"question": question or f"Error generating Q{i+1}", "answer": "N/A"}
                    #     )
                    # else:
                    #     tagger = EvidenceTagger(
                    #         question=question, answer=answer, enriched_claim=enriched_claim
                    #     )
                    #     ev_type = tagger.run(self.model_router.get_model())
                    #     qa_branch.append(
                    #         {"question": question, "answer": answer, "evidence_type": ev_type}
                    #     )

                global_qa_pairs.extend(qa_branch)

                direct, background, total = score_qa_evidence_support(
                    qa_branch, enriched_claim
                )
                if self.verbose:
                    logging.info(
                        f"Evidence Score ‚Üí Direct: {direct}, Background: {background}, "
                        f"Total: {total}"
                    )

                # optional: select best QA in branch
                if num_claims_per_branch > 1 and qa_branch:
                    selector = QASelectionTool(qa_pairs=qa_branch)
                    _, _ = selector.run(self.model_router.get_model())


                # --- Optional: Step 3 - Select best QA from this branch ---
                selected_branch_pair = None
                if num_claims_per_branch > 1 and qa_branch:
                   if self.verbose: logging.info(f"Selecting best QA pair for Chain {chain_idx + 1}...")
                   selector = QASelectionTool(qa_pairs=qa_branch)
                   selected_branch_pair, select_success = selector.run(self.model_router.get_model())
                   if select_success and selected_branch_pair:
                        if self.verbose: logging.info(f"Selected QA: {selected_branch_pair}")

            results["qa_pairs"] = global_qa_pairs

            # Evidence sufficiency check
            total_direct, _, _ = score_qa_evidence_support(global_qa_pairs, enriched_claim)

            if total_direct == 0:
                logging.warning("Insufficient direct evidence. Skipping classification.")
                results["status"] = "Uncertain"
                results["final_decision"] = "Uncertain"
                results["explanation"] = "No direct Q&A evidence supporting or refuting the claim was found."
                return results

            relevant_qa_pairs = [
                  qa for qa in global_qa_pairs if qa.get("evidence_type") in ("supports", "refutes")
              ]

            # --- Step 4: Preliminary Veracity Classification ---
            if self.verbose: logging.info("Step 4: Running Preliminary Veracity Classification...")
            if global_qa_pairs:
                 veracity_classifier = VeracityClassifier(
                     enriched_claim=enriched_claim,
                     image_context=image_context,
                     qa_pairs=global_qa_pairs
                 )
                 prelim_veracity, prelim_reason, prelim_success = veracity_classifier.run(self.model_router.get_model())
                 time.sleep(timers)
                 results["preliminary_veracity"] = prelim_veracity
                 results["preliminary_reason"] = prelim_reason
                 if not prelim_success:
                      logging.warning(f"Preliminary veracity check failed or produced error: {prelim_reason}")
                      results["preliminary_veracity"] = "Error"
                 elif self.verbose:
                      logging.info(f"Preliminary Veracity: {prelim_veracity}, Reason: {prelim_reason}")
            else:
                 logging.warning("Skipping preliminary veracity check as no Q&A pairs were generated.")
                 results["preliminary_veracity"] = "Skipped - No Q&A"
                 results["preliminary_reason"] = "No Q&A pairs were generated or answered successfully."

            # --- Step 4.5: Soft fallback check ---
            score = evidence_strength_score(global_qa_pairs, enriched_claim)

            if score < 0.3:  # threshold can be tuned (e.g. 0.2 to 0.4)
                logging.warning(f"Evidence score {score:.2f} too weak. Returning 'Uncertain'.")
                results["status"] = "Uncertain"
                results["final_decision"] = "Uncertain"
                results["explanation"] = "Insufficient direct evidence to make a confident classification."
                return results

            # --- Step 5: Final Classification ---
            if self.verbose: logging.info("Step 5: Running Final Classification...")
            final_classifier = FinalClassifier(
                relevancy_response=results["relevancy_result"],
                relevancy_confidence=results["relevancy_confidence"],
                enriched_claim=results["enriched_claim"],
                image_context=results["image_context"],
                qa_pairs=results["qa_pairs"],
                preliminary_veracity=results["preliminary_veracity"],
                preliminary_reason=results["preliminary_reason"]
            )
            final_decision, explanation, final_success = final_classifier.run(self.model_router.get_model())
            time.sleep(timers)

            results["final_decision"] = final_decision
            results["explanation"] = explanation

            if not final_success:
              if final_decision == "Uncertain":
                  results["status"] = "Uncertain"
              else:
                  results["status"] = "Failed - Final Classification Error"
                  results["error_details"] = explanation
                  logging.error(f"{results['status']} for {image_path}. Reason: {explanation}")

            elif final_decision in ["Misinformation", "Not Misinformation", "Uncertain"]:
                 results["status"] = "Success"
            elif final_decision == "Model Refusal":
                 results["status"] = "Failed - Model Refusal"
                 logging.warning(f"Pipeline ended with Model Refusal for {image_path}.")
            elif final_decision == "Format Error":
                 results["status"] = "Failed - Final Format Error"
                 logging.warning(f"Pipeline ended with Final Classification Format Error for {image_path}.")
            else:
                 results["status"] = f"Failed - {final_decision}"
                 logging.error(f"Pipeline failed with unexpected final decision state: {final_decision} for {image_path}.")

            if self.verbose:
                 logging.info(f"--- Final Result ---")
                 logging.info(f"Status: {results['status']}")
                 logging.info(f"Final Decision: {results['final_decision']}")
                 logging.info(f"Explanation: {results['explanation']}")

            return results

        except Exception as e:
            logging.exception(f"Unhandled exception in AdvancedRAGPipeline for {image_path}: {e}")
            results["status"] = "Failed - Unhandled Exception"
            results["error_details"] = str(e)
            results["final_decision"] = "Pipeline Error"
            results["explanation"] = f"An unexpected error occurred: {e}"
            return results

# --------------------------------------------------------------------------- #
#  M M F a k e B e n c h   B e n c h m a r k e r                              #
#                                                                           #
#  Works **without** any external CSV. Ground-truth labels come straight    #
#  from MMFakeBenchDataset:                                                 #
#     ‚Ä¢ label_bin   ‚Üí 0 = True  (Not-Misinformation)                        #
#                     1 = Fake (Misinformation)                             #
#     ‚Ä¢ label_multi ‚Üí optional fine-grained class (unused here)             #
# --------------------------------------------------------------------------- #
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
)
from torch.utils.data import DataLoader as TorchDataLoader
from IPython.display import display, Image as IPImage

def run_misinformation_benchmark(
    pipeline: "AdvancedRAGPipeline",
    dataset: "MMFakeBenchDataset",
    pipeline_params: dict | None = None,
) -> None:
    """
    Evaluate the pipeline on MMFakeBench.

    - Binary ground truth: 0 ‚Üí Not-Misinformation, 1 ‚Üí Misinformation
    - Accepts pipeline outputs:  {Not Misinformation, Misinformation, Uncertain}
    - Prints accuracy, full report, 3√ó3 confusion matrix (Uncertain kept)
    - Saves a detailed CSV for later inspection
    """
    logging.info("üîé  Starting MMFakeBench benchmark‚Ä¶")

    # ------------------------------------------------------------------ #
    # 0  defaults                                                        #
    # ------------------------------------------------------------------ #
    if pipeline_params is None:
        pipeline_params = {"num_claims_per_branch": 1, "num_chains": 2}

    LABEL_SET = ["Not Misinformation", "Misinformation", "Uncertain"]

    # ------------------------------------------------------------------ #
    # 1  iterate over dataset                                            #
    # ------------------------------------------------------------------ #
    y_true, y_pred, details = [], [], []
    failures = 0

    loader = TorchDataLoader(dataset, batch_size=1, shuffle=False)
    total_samples = len(dataset)
    logging.info("Benchmarking on %d samples‚Ä¶", total_samples)

    for idx, batch in enumerate(loader, start=1):
        # batch is a tuple of tensors / lists with length 1
        image_path, caption, y_raw = batch[0][0], batch[1][0], batch[2][0]

        gt_label = "Not Misinformation" if y_raw == "True" else "Misinformation"

        logging.info("‚Äî Sample %d/%d ‚Äî", idx, total_samples)
        logging.info("GT label : %s", gt_label)
        logging.info("Caption  : %s", caption[:120])
        try:
            display(IPImage(filename=image_path, width=280))
        except Exception:
            pass

        # run pipeline
        result = pipeline.run_pipeline(
            image_path=image_path,
            headline=caption,
            **pipeline_params,
        )
        pred = result.get("final_decision", "Pipeline Error")
        logging.info("Predicted: %s", pred)

        if pred in LABEL_SET:
            y_true.append(gt_label)
            y_pred.append(pred)
        else:
            failures += 1

        details.append(
            {
                "index": idx,
                "image_path": image_path,
                "caption": caption,
                "ground_truth": gt_label,
                "prediction": pred,
                "status": result.get("status", "Unknown"),
                "explanation": result.get("explanation", ""),
            }
        )

    # ------------------------------------------------------------------ #
    # 2  reporting                                                       #
    # ------------------------------------------------------------------ #
    logging.info(
        "\n‚Äî Benchmark complete ‚Äî\nTotal: %d | Failures: %d | Evaluated: %d",
        total_samples,
        failures,
        len(y_true),
    )

    if y_true:
        print("\nClassification report:")
        print(
            classification_report(
                y_true,
                y_pred,
                labels=LABEL_SET,
                target_names=LABEL_SET,
                zero_division=0,
            )
        )

        cm = confusion_matrix(y_true, y_pred, labels=LABEL_SET)
        cm_df = pd.DataFrame(
            cm,
            index=[f"True ‚Üí {l}" for l in LABEL_SET],
            columns=[f"Pred ‚Üí {l}" for l in LABEL_SET],
        )
        print("\nConfusion matrix (counts):")
        print(cm_df)

        row_pct = (cm / cm.sum(axis=1, keepdims=True) * 100).round(1)
        pct_df = pd.DataFrame(
            row_pct,
            index=[f"True ‚Üí {l}" for l in LABEL_SET],
            columns=[f"Pred ‚Üí {l}" for l in LABEL_SET],
        )
        print("\nConfusion matrix (% within each true label):")
        print(pct_df)

        print(f"\nAccuracy: {accuracy_score(y_true, y_pred):.4f}")
    else:
        print("No valid predictions collected; nothing to evaluate.")

    # ------------------------------------------------------------------ #
    # 3  persist results                                                 #
    # ------------------------------------------------------------------ #
    try:
        pd.DataFrame(details).to_csv("mmfakebench_results.csv", index=False)
        logging.info("Detailed results saved to mmfakebench_results.csv")
    except Exception as e:
        logging.error("Could not save results CSV: %s", e)

# --------------------------------------------------------------------------- #
#  Quick visual sanity-check for MMFakeBench                                  #
# --------------------------------------------------------------------------- #
def preview_sampled_data(dataset: "MMFakeBenchDataset", num_preview: int = 5) -> None:
    """
    Show a few (image, caption) pairs with their ground-truth labels.

    Each item from MMFakeBenchDataset returns:
        image_path, caption_text, label_bin, label_multi
          ‚Ä¢ label_bin   : 0 = True  (Not-Misinformation)
                          1 = Fake (Misinformation)
          ‚Ä¢ label_multi : 'original' | 'mismatch' |
                          'textual_veracity_distortion' |
                          'visual_veracity_distortion'
    """
    from IPython.display import display, Image as IPImage

    n = min(num_preview, len(dataset))
    print(f"\nüìå Previewing {n} of {len(dataset)} samples:\n")

    for idx in range(n):
        img_path, caption, y_bin, y_multi, img_src, text_src,  = dataset[idx]

        lbl_bin  = "Not Misinformation" if y_bin == 0 else "Misinformation"
        lbl_multi = y_multi

        print(f"üîπ Sample {idx + 1}")
        print(f"Binary label : {lbl_bin}")
        print(f"Fine label   : {lbl_multi}")
        print(f"Caption      : {caption}")
        print(f"Image path   : {img_path}")
        print(f"Image source : {img_src}")
        print(f"Text source  : {text_src}")

        try:
            display(IPImage(filename=img_path, width=300))
        except Exception as exc:
            print(f"‚ö†Ô∏è  Could not display image: {exc}")

        print("-" * 60)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  M a i n   e n t r y   ( M M F a k e B e n c h  v e r s i o n )
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main(
    model_name: str = "gemini-2.0-flash",
    model_params: dict | None = None,
    pipeline_params: dict | None = None,
    dataset_config: dict | None = None,
    preview_only: bool = False,
    num_preview: int = 5,
):
    logging.info("üöÄ  MMFakeBench run started")

    # ------------------------------------------------------------------ #
    # 0  default configs                                                 #
    # ------------------------------------------------------------------ #
    _DEFAULT_DATASET_CONFIG = {
        "images_dir": "/content/drive/MyDrive/MMFakeBench/MMFakeBench_test/MMFakeBench_test",            # root folder that holds /real/ and /fake/
        "json_path":  "/content/drive/MyDrive/MMFakeBench/MMFakeBench_test.json",
        "limit": 10,                         # None = full set
        "seed": 42,
    }
    _DEFAULT_PIPELINE_PARAMS = {"num_claims_per_branch": 3, "num_chains": 3}
    _DEFAULT_MODEL_PARAMS = {
        "openai_api_key": openai_key,
        "google_api_key": google_key,
        "temperature": 0.2,
    }

    dataset_config  = dataset_config  or _DEFAULT_DATASET_CONFIG
    pipeline_params = pipeline_params or _DEFAULT_PIPELINE_PARAMS
    model_params    = model_params    or _DEFAULT_MODEL_PARAMS

    img_root = dataset_config["images_dir"]
    json_path = dataset_config["json_path"]

    if not os.path.isdir(img_root):
        logging.error("Images directory not found: %s", img_root)
        return
    if not os.path.isfile(json_path):
        logging.error("JSON file not found: %s", json_path)
        return

    logging.info("Dataset config : %s", dataset_config)
    logging.info("Pipeline config: %s", pipeline_params)

    # ------------------------------------------------------------------ #
    # 1  initialise model-router + dataset                               #
    # ------------------------------------------------------------------ #
    try:
        if not (model_params.get("openai_api_key") or model_params.get("google_api_key")):
            raise ValueError("No API key provided")

        model_router = ModelRouter(model_name=model_name, **model_params)

        dataset = MMFakeBenchDataset(
            json_path=json_path,
            images_base_dir=img_root,
            limit=dataset_config.get("limit"),
        )
        if len(dataset) == 0:
            logging.error("Dataset is empty ‚Äî check paths and limit")
            return

        if preview_only:
            preview_sampled_data(dataset, num_preview)

        pipeline = AdvancedRAGPipeline(
            model_router=model_router,
            brave_api_key=brave_key,
            verbose=True,
        )

    except Exception as exc:
        logging.exception("Initialisation failed: %s", exc)
        return

    # ------------------------------------------------------------------ #
    # 2  run benchmark                                                   #
    # ------------------------------------------------------------------ #
    try:
        run_misinformation_benchmark(
            pipeline=pipeline,
            dataset=dataset,
            pipeline_params=pipeline_params,
        )
    except Exception as exc:
        logging.exception("Benchmark errored: %s", exc)

    logging.info("üèÅ  MMFakeBench run finished")

# base_drive_path = "/content/drive/MyDrive/MMFakeBench/MMFakeBench_test"  # adjust if yours differs

# my_dataset_config = {
#     "images_dir": base_drive_path,                          # contains /real/ and /fake/ sub-folders
#     "json_path":  os.path.join(base_drive_path, "MMFakeBench_test.json"),
#     "limit": 50,                                            # None = evaluate entire split
#     "seed": 42
# }

my_pipeline_params = {
    "num_claims_per_branch": 3,
    "num_chains": 3
}

my_model_params = {
    "google_api_key": google_key,   # or "openai_api_key": openai_key
    "temperature": 0.2
}

# preview five random examples without running the full pipeline
main(
    model_name="gemini-2.0-flash",          # or "gpt-4o-mini"
    model_params=my_model_params,
    pipeline_params=my_pipeline_params,
    preview_only=True,                      # True = just show images + captions
    num_preview=5
)

# preview five random examples without running the full pipeline
main(
    model_name="gemini-2.0-flash",          # or "gpt-4o-mini"
    model_params=my_model_params,
    pipeline_params=my_pipeline_params,
    preview_only=True,                      # True = just show images + captions
    num_preview=5
)

# preview five random examples without running the full pipeline
main(
    model_name="gemini-2.0-flash",          # or "gpt-4o-mini"
    model_params=my_model_params,
    pipeline_params=my_pipeline_params,
    preview_only=True,                      # True = just show images + captions
    num_preview=5
)

my_dataset_config = {
          "images_dir": os.path.join(base_drive_path, "images"),
          "corpus_csv": os.path.join(base_drive_path, "train/Corpus2.csv"),
          "n_topics": 100,
          "max_samples_per_topic": 1,
          "limit": 100,
          "seed": 42
      }

      my_pipeline_params = {
          "num_claims_per_branch": 2,
          "num_chains": 2
      }

      my_model_params = {
          "google_api_key": google_key,
          "temperature": 0.2
      }

      main(
          model_name="gemini-2.0-flash",
          model_params=my_model_params,
          pipeline_params=my_pipeline_params,
          dataset_config=my_dataset_config,
          preview_only=True,      # Set to False to run full benchmark
          num_preview=100
      )

# """#  need to

# *   fix the ground truth. It is not showing.
# *   implement Brave Search to compare results.


# """

if __name__ == "__main__":
    base_drive_path = "/content/drive/MyDrive/mocheg"
    my_dataset_config = {
        "images_dir": os.path.join(base_drive_path, "images"),
        "img_evidence_csv": os.path.join(base_drive_path, "train/img_evidence_qrels.csv"),
        "corpus_csv": os.path.join(base_drive_path, "train/Corpus2.csv"),
        "n_topics": 10,
        "max_samples_per_topic": 1,
        "limit": 200,
        "seed": 123
    }

    my_pipeline_params = {
        "num_claims_per_branch": 3,
        "num_chains": 3
    }

    my_model_params = {
        "google_api_key": google_key,
        "temperature": 0.2,
    }

    main(
        model_name="gemini-2.0-flash",
        model_params=my_model_params,
        pipeline_params=my_pipeline_params,
        dataset_config=my_dataset_config
    )